# ğŸ§  Vision Transformer Explainability: LeGrad Vs GMAR vs Classical Rollout

This project provides a unified framework for **interpreting Vision Transformers (ViTs)** using three state-of-the-art explanation methods:

- ğŸ¯ **LeGrad** â€” An Explainability Method for Vision Transformers via Feature Formation Sensitivity
- ğŸ” **GMAR** â€” Gradient-weighted Multi-head Attention Rollout with L1 and L2 norms
- ğŸ§± **Classical Rollout** â€” Recursive layer-wise attention propagation (Abnar & Zuidema, 2020)

These methods generate **visual attention heatmaps** to understand **which regions the model focuses on** during prediction. The project also supports **quantitative evaluation** using segmentation masks.

---

## ğŸ“‚ Project Structure

```
project/
â”œ images/                # Input images for classification and visualization
â”œ masks/                # Ground truth binary masks for evaluation (same name as input image, .png format)
â”œ results/               # Output attention heatmaps
â”‚  â”œ legrad/             # LeGrad overlays
â”‚  â”œ gmar_l1/            # GMAR (L1 norm) overlays
â”‚  â”œ gmar_l2/            # GMAR (L2 norm) overlays
â”‚  â”” rollout/            # Classical Rollout overlays
â”œ src/                   # Source files
â”‚  â”œ vit.py              # Custom ViT wrapper with attention extraction
â”‚  â”œ legrad.py           # LeGrad method implementation
â”‚  â”œ gmar.py             # GMAR method implementation
â”‚  â”œ rollout.py          # Classical rollout implementation
â”‚  â”œ metrics.py          # PixelAcc, IoU, and mAP metrics
â”‚  â”” __pycache__/        # Python cache
â”œ main.py                # Run visualization and evaluation
â”œ readme.md              # This file
```

---

## ğŸš€ How to Use

### 1. Place input images

- Put images into the `images/` folder.
- Add binary masks (same base filename, `.png`) into the `masks/` folder.

### 2. Run the project

```bash
python main.py
```

Choose one of the methods when prompted:

```bash
Which method? (legrad/gmar/rollout):
```

### 3. View and evaluate results

- Heatmaps will be saved in `results/<method>/`
- If masks are present, the script automatically computes:
  - **Pixel Accuracy**
  - **Mean Intersection over Union (mIoU)**
  - **Mean Average Precision (mAP)**

---

## ğŸ“Š Quantitative Evaluation

The project evaluates interpretability performance using three metrics:

- **Pixel Accuracy**: Proportion of pixels correctly classified
- **mIoU**: Intersection over Union between predicted and ground-truth masks
- **mAP**: Pixel-wise mean Average Precision

These are computed after binarizing the heatmap using a **threshold of 0.5**.

---

## ğŸ§ª Example Outputs

**Heatmaps:**

```
results/
â”œ legrad/2007_000720_legrad.png
â”œ gmar_l1/2007_000720_gmar.png
â”œ gmar_l2/2007_000720_gmar.png
â”œ rollout/2007_000720_rollout.png
```

**Metrics Output:**
```
PixelAcc: 0.88 | IoU: 0.53 | AP: 0.85
```

---

## âš™ï¸ Model Details

- Base model: `google/vit-base-patch16-224` (from Hugging Face)
- Pretrained on ImageNet-1K
- No fine-tuning; all explanations use inference-only

---

## ğŸ“¦ Installation

Install dependencies:

```bash
pip install torch torchvision transformers matplotlib pillow numpy scikit-learn
```

---

## ğŸ“š References

- [LeGrad: An Explainability Method for Vision Transformers (2024)](https://arxiv.org/pdf/2404.03214)
- [GMAR: Gradient-weighted Multi-head Attention Rollout (2025)](https://arxiv.org/pdf/2504.19414)
- [Quantifying Attention Flow in Transformers (2020)](https://arxiv.org/pdf/2005.00928)

---

## âœ¨ Author

Prepared by **Soham Joita**  
Masterâ€™s student, OTH Amberg-Weiden  
AI for Industrial Applications ğŸ’¡
